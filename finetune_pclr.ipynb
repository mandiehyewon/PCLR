{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c97a3114",
   "metadata": {},
   "source": [
    "# Finetuning PCLR model trained without Apollo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637318d",
   "metadata": {},
   "source": [
    "# Load PCLR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f608ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Date: Jan 3rd 2023\n",
    "Author: Hyewon Jeong\n",
    "'''\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54cae435",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Needs to be debugged sometime:\n",
    "Maybe need to specify getx and gety function based on the datagen class below?\n",
    "https://medium.com/analytics-vidhya/write-your-own-custom-data-generator-for-tensorflow-keras-1252b64e41c3\n",
    "'''\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, df_tab, uids, to_fit=True, batch_size=64, ecg_len = 2500,\n",
    "                 n_channels=12, n_classes=2, shuffle=True):\n",
    "        self.dir_csv = '/storage/shared/apollo/same-day/'\n",
    "        self.pcwp_th = 18.0\n",
    "        self.df = df_tab\n",
    "        self.pcwp_train = np.load(\"/storage/hyewonjeong/metricssl_02/stores/train_info.npy\")\n",
    "        self.pcwp_mean = self.pcwp_train[0]\n",
    "        self.pcwp_std = self.pcwp_train[1]\n",
    "        self.label = 'pcwp'\n",
    "        self.train_mode = 'classification' # 'regression', otherwise\n",
    "        \n",
    "        self.list_IDs = uids\n",
    "        self.to_fit = to_fit\n",
    "        self.batch_size = batch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\n",
    "        :return: number of batches per epoch\n",
    "        \"\"\"\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # load ECG\n",
    "        qid = row['QuantaID']\n",
    "        doc = row['Date_of_Cath']\n",
    "        fname = os.path.join(self.dir_csv, f'{qid}_{doc}.csv')\n",
    "\n",
    "        x = pd.read_csv(fname).values[::2,1:].astype(np.float32)\n",
    "        x = x / 1000\n",
    "        \n",
    "        if not self.to_fit:\n",
    "            return X\n",
    "        \n",
    "        if self.label == 'pcwp':\n",
    "            if self.train_mode == 'regression':\n",
    "                y = row['PCWP_mean']\n",
    "                y = (y-self.pcwp_mean)/(self.pcwp_std) # normalize labels\n",
    "            else:\n",
    "                y = row['PCWP_mean'] > self.pcwp_th\n",
    "        elif self.label == 'age':\n",
    "            if self.train_mode == 'regression':\n",
    "                y = row['Age_at_Cath'] #regression\n",
    "            else:\n",
    "                y = row['PCWP_mean'] > self.args.pcwp_th\n",
    "\n",
    "        elif self.label == 'gender':\n",
    "            y = row['Sex']\n",
    "\n",
    "        return x[:2496,:].T, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e956fef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'rank'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_941937/2857834492.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Train model on dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/hyewonj/envs/pclr/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hyewonj/envs/pclr/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hyewonj/envs/pclr/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hyewonj/envs/pclr/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    796\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m     \u001b[0moutput_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_dynamic_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m     \u001b[0moutput_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hyewonj/envs/pclr/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hyewonj/envs/pclr/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hyewonj/envs/pclr/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_get_dynamic_shape\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    792\u001b[0m       \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m       \u001b[0;31m# Unknown number of dimensions, `as_list` cannot be called.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'rank'"
     ]
    }
   ],
   "source": [
    "# Set Seeds and Devices (GPU)\n",
    "seed = 0\n",
    "gpu = 0\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "# Load Dataset\n",
    "dir_data = '/storage/shared/apollo/same-day/'\n",
    "tab = '/storage/shared/apollo/same-day/tabular_data.csv'\n",
    "df_tab = pd.read_csv(tab)\n",
    "df_tab = df_tab.dropna(subset=['CO'])\n",
    "frac_train = 0.8\n",
    "frac_val = 0.2\n",
    "uids = df_tab['QuantaID'].unique()\n",
    "\n",
    "train_ids = np.load(\"/storage/hyewonjeong/metricssl_02/stores/train_ids.npy\")\n",
    "val_ids = np.load(\"/storage/hyewonjeong/metricssl_02/stores/val_ids.npy\")\n",
    "test_ids = np.load(\"/storage/hyewonjeong/metricssl_02/stores/test_ids.npy\")\n",
    "\n",
    "training_generator = DataGenerator(df_tab, train_ids)\n",
    "validation_generator = DataGenerator(df_tab, val_ids)\n",
    "test_generator = DataGenerator(df_tab, val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "216b7538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DataGenerator at 0x7fdf1d9e9890>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbff7557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design model\n",
    "model = load_model(\"./PCLR_wo_apollo.h5\")\n",
    "model.compile()\n",
    "\n",
    "# Train model on dataset\n",
    "model.fit(training_generator, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b398c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'path to images'\n",
    "\n",
    "pred_labels = [...] # list of image names\n",
    "\n",
    "pred_generator = DataGenerator(pred_idx, pred_labels, image_path, to_fit=False)\n",
    "\n",
    "pred = model.predict_generator(pred_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e64b5b3",
   "metadata": {},
   "source": [
    "# Codes to 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b42f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model for training\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    " \n",
    "# Train the model\n",
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=\n",
    "         train_generator.samples/train_generator.batch_size,\n",
    "      epochs=20,\n",
    "      validation_data=validation_generator, \n",
    "      validation_steps=\n",
    "         validation_generator.samples/validation_generator.batch_size,\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faa827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for plotting of the model results\n",
    "def visualize_results(history):\n",
    "    # Plot the accuracy and loss curves\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    " \n",
    "    epochs = range(len(acc))\n",
    " \n",
    "    plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    " \n",
    "    plt.figure()\n",
    " \n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    " \n",
    "    plt.show()\n",
    " \n",
    " \n",
    "# Run the function to illustrate accuracy and loss\n",
    "visualize_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c2bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for obtaining of the errors \n",
    "def obtain_errors(val_generator, predictions):\n",
    "    # Get the filenames from the generator\n",
    "    fnames = validation_generator.filenames\n",
    " \n",
    "    # Get the ground truth from generator\n",
    "    ground_truth = validation_generator.classes\n",
    " \n",
    "    # Get the dictionary of classes\n",
    "    label2index = validation_generator.class_indices\n",
    " \n",
    "    # Obtain the list of the classes\n",
    "    idx2label = list(label2index.keys())\n",
    "    print(\"The list of classes: \", idx2label)\n",
    " \n",
    "    # Get the class index\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    " \n",
    "    errors = np.where(predicted_classes != ground_truth)[0]\n",
    "    print(\"Number of errors = {}/{}\".format(len(errors),validation_generator.samples))\n",
    "     \n",
    "    return idx2label, errors, fnames\n",
    " \n",
    " \n",
    "# Utility function for visualization of the errors\n",
    "def show_errors(idx2label, errors, predictions, fnames):\n",
    "    # Show the errors\n",
    "    for i in range(len(errors)):\n",
    "        pred_class = np.argmax(predictions[errors[i]])\n",
    "        pred_label = idx2label[pred_class]\n",
    " \n",
    "        title = 'Original label:{}, Prediction :{}, confidence : {:.3f}'.format(\n",
    "            fnames[errors[i]].split('/')[0],\n",
    "            pred_label,\n",
    "            predictions[errors[i]][pred_class])\n",
    " \n",
    "        original = load_img('{}/{}'.format(validation_dir,fnames[errors[i]]))\n",
    "        plt.figure(figsize=[7,7])\n",
    "        plt.axis('off')\n",
    "        plt.title(title)\n",
    "        plt.imshow(original)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db4ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions from the model using the generator\n",
    "predictions = model.predict(validation_generator, steps=validation_generator.samples/validation_generator.batch_size,verbose=1)\n",
    " \n",
    "# Run the function to get the list of classes and errors\n",
    "idx2label, errors, fnames = obtain_errors(validation_generator, predictions)\n",
    " \n",
    "# Run the function to illustrate the error cases\n",
    "show_errors(idx2label, errors, predictions, fnames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
