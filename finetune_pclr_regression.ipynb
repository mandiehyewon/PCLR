{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c1c0097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 22:52:56.457786: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-28 22:52:56.866334: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-28 22:52:58.305573: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/storage/araghu/.conda/envs/hfnet/lib/\n",
      "2023-03-28 22:52:58.305927: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/storage/araghu/.conda/envs/hfnet/lib/\n",
      "2023-03-28 22:52:58.305956: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, average_precision_score, f1_score, accuracy_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from scipy import stats\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from hnet import AppendNet\n",
    "\n",
    "def load_pretrained_model(pre_trained_loc=\"./PCLR.h5\") :\n",
    "    pre_trained_model = load_model(pre_trained_loc)\n",
    "    \n",
    "    return pre_trained_model\n",
    "\n",
    "def do_bootstrap_regression(preds, trues, n=1000):\n",
    "    rmse_list = []\n",
    "    r_list = []\n",
    "    pval_list = []\n",
    "\n",
    "    rng = np.random.RandomState(seed=1)\n",
    "    for _ in range(n):\n",
    "        idxs = rng.choice(len(trues), size=len(trues), replace=True)\n",
    "        pred_arr = preds[idxs]\n",
    "        true_arr = trues[idxs]\n",
    "\n",
    "        rmse = rmse_loss(pred_arr, true_arr)\n",
    "        r, pval = stats.pearsonr(true_arr, pred_arr)\n",
    "\n",
    "        rmse_list.append(rmse)\n",
    "        r_list.append(r)\n",
    "        pval_list.append(pval)\n",
    "\n",
    "    return np.array(rmse_list), np.array(r_list), np.array(pval_list)\n",
    "\n",
    "def confidence_interval(values, alpha=0.95):\n",
    "    lower = np.percentile(values, (1-alpha)/2 * 100)\n",
    "    upper = np.percentile(values, (alpha + (1-alpha)/2) * 100)\n",
    "    return lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "249d61c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ecg(df):\n",
    "    ecgs = []\n",
    "    for idx in df.index:\n",
    "        row = df.loc[idx]\n",
    "        qid = row['QuantaID']\n",
    "        doc = row['Date_of_Cath']\n",
    "        fname = f'/storage/shared/apollo/same-day/{qid}_{doc}.csv'\n",
    "        x = pd.read_csv(fname).values[...,1:].astype(np.float32)\n",
    "        x /= 1000\n",
    "        x = x[:4096, :].T\n",
    "        ecgs.append(x)\n",
    "        \n",
    "    ecgs = np.array(ecgs)\n",
    "    return np.transpose(ecgs, (0,2,1))\n",
    "\n",
    "def get_data(batch_size=64):\n",
    "    df_tab = pd.read_csv(os.path.join('/storage/shared/apollo/same-day/tabular_data.csv'))\n",
    "    train_ids = np.load(\"./stores/train_ids.npy\")\n",
    "    val_ids = np.load(\"./stores/val_ids.npy\")\n",
    "    test_ids = np.load(\"./stores/test_ids.npy\")\n",
    "\n",
    "    train_ids = train_ids[len(train_ids) // 2 :]\n",
    "    val_ids = val_ids[len(val_ids) // 2 :]\n",
    "    test_ids = test_ids[len(test_ids) // 2 :]\n",
    "\n",
    "    train_df = df_tab[df_tab[\"QuantaID\"].isin(train_ids)]\n",
    "    val_df = df_tab[df_tab[\"QuantaID\"].isin(val_ids)]\n",
    "    test_df = df_tab[df_tab[\"QuantaID\"].isin(test_ids)]\n",
    "    print(len(train_df), len(val_df), len(test_df))\n",
    "\n",
    "    X_train = get_ecg(train_df)\n",
    "    X_val = get_ecg(val_df)\n",
    "    X_test = get_ecg(test_df)\n",
    "\n",
    "    y_train = train_df[\"PCWP_mean\"].values\n",
    "    y_val = val_df[\"PCWP_mean\"].values\n",
    "    y_test = test_df[\"PCWP_mean\"].values\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "def rmse_loss(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25647daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "2442 893 923\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "pre_trained_model = load_pretrained_model(pre_trained_loc='./PCLR.h5')\n",
    "latent = tf.keras.Model(pre_trained_model.inputs, pre_trained_model.get_layer('embed').output)\n",
    "full_model = AppendNet(latent, new_layers = [128, 1], classification=False) # same hidden dimension as dml (128)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam() # can modify LR, of course\n",
    "# loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "loss_fn = rmse_loss\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0df34dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "77/77 [==============================] - 12s 99ms/step - loss: 15.0063\n",
      "Epoch 2/50\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.9235\n",
      "Epoch 3/50\n",
      "77/77 [==============================] - 11s 147ms/step - loss: 13.7081\n",
      "Epoch 4/50\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.2645\n",
      "Epoch 5/50\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 13.6062\n",
      "Epoch 6/50\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.4615\n",
      "Epoch 7/50\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 13.6717\n",
      "Epoch 8/50\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.6344\n",
      "Epoch 9/50\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 13.0034\n",
      "Epoch 10/50\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 13.3835\n",
      "Epoch 11/50\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.4399\n",
      "Epoch 12/50\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.2835\n",
      "Epoch 13/50\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.4502\n",
      "Epoch 14/50\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 13.2518\n",
      "Epoch 15/50\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 13.3472\n",
      "Epoch 16/50\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.2949\n",
      "Epoch 17/50\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.6410\n",
      "Epoch 18/50\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.3999\n",
      "Epoch 19/50\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 13.4393\n",
      "Epoch 20/50\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.2546\n",
      "Epoch 21/50\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.9142\n",
      "Epoch 22/50\n",
      "77/77 [==============================] - 8s 109ms/step - loss: 13.0815\n",
      "Epoch 23/50\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.3352\n",
      "Epoch 24/50\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.4429\n",
      "Epoch 25/50\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.0688\n",
      "Epoch 26/50\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.4846\n",
      "Epoch 27/50\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.3973\n",
      "Epoch 28/50\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.2123\n",
      "Epoch 29/50\n",
      "77/77 [==============================] - 11s 142ms/step - loss: 12.8275\n",
      "Epoch 30/50\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 12.9884\n",
      "Epoch 31/50\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.1670\n",
      "Epoch 32/50\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.1047\n",
      "Epoch 33/50\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.0361\n",
      "Epoch 34/50\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 13.1115\n",
      "Epoch 35/50\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.3135\n",
      "Epoch 36/50\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.1893\n",
      "Epoch 37/50\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 13.0085\n",
      "Epoch 38/50\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 13.3287\n",
      "Epoch 39/50\n",
      "77/77 [==============================] - 11s 146ms/step - loss: 12.9859\n",
      "Epoch 40/50\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.2351\n",
      "Epoch 41/50\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.2503\n",
      "Epoch 42/50\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.0607\n",
      "Epoch 43/50\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 13.1014\n",
      "Epoch 44/50\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.9148\n",
      "Epoch 45/50\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.9175\n",
      "Epoch 46/50\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 12.9721\n",
      "Epoch 47/50\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 13.0160\n",
      "Epoch 48/50\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 12.8494\n",
      "Epoch 49/50\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.0769\n",
      "Epoch 50/50\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.0411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f46f85fc790>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50\n",
    "full_model.compile(optimizer, loss_fn)\n",
    "full_model.fit(X_train, y_train, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3817940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 12s 98ms/step - loss: 15.5466\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 14.0876\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.7544\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.3075\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 13.6482\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.4675\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 8s 107ms/step - loss: 13.6814\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 13.6239\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.0276\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 12s 153ms/step - loss: 13.3434\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.4667\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 13.3233\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 13.4278\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.3000\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.3789\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 13.2924\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 13.6598\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.3597\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.4645\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.3006\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 10s 134ms/step - loss: 12.9523\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 13.0665\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 13.3334\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.4333\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.0611\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 13.5019\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 13.4934\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 13.2343\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 12.8177\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 12.9986\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.1478\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 13.0556\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.0732\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.1311\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 13.3395\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 8s 107ms/step - loss: 13.1394\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 13.0046\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 10s 135ms/step - loss: 13.3607\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 13.0111\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 13.2548\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 13.2574\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.0958\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 13.0347\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.9225\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.8290\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.0253\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.9920\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.8467\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.0914\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.0302\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.1300\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.9048\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 10s 131ms/step - loss: 12.8131\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.0348\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.1217\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.1212\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 13.0153\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.2008\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.9011\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 13.0395\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.9264\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 8s 106ms/step - loss: 12.9217\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.8722\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 8s 108ms/step - loss: 13.0827\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 8s 106ms/step - loss: 12.7474\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 13.0094\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 11s 146ms/step - loss: 13.0309\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 12.9750\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.9699\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.0543\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 12.6894\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 9s 111ms/step - loss: 12.9667\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.0114\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.5775\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.8111\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 12.9127\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 12.9581\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.9111\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 12.7335\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.8286\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.8650\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.4084\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 11s 143ms/step - loss: 13.2306\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 12.8510\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 12.9351\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 8s 107ms/step - loss: 12.7872\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 12.6761\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 13.0014\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 12.7152\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 12.8490\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 12.7734\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 13.1572\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 12.7508\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.8314\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.9321\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 13.0483\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 10s 137ms/step - loss: 12.8954\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 8s 102ms/step - loss: 12.7541\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.9781\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.9663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f46e87e1280>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "pre_trained_model = load_pretrained_model(pre_trained_loc='./PCLR.h5')\n",
    "latent = tf.keras.Model(pre_trained_model.inputs, pre_trained_model.get_layer('embed').output)\n",
    "full_model = AppendNet(latent, new_layers = [128, 1], classification=False)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam() # can modify LR, of course\n",
    "# loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "loss_fn = rmse_loss\n",
    "full_model.compile(optimizer, loss_fn)\n",
    "full_model.fit(X_train, y_train, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c0c7932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "77/77 [==============================] - 12s 104ms/step - loss: 14.9223\n",
      "Epoch 2/150\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.9433\n",
      "Epoch 3/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.6449\n",
      "Epoch 4/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 13.2819\n",
      "Epoch 5/150\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 13.5724\n",
      "Epoch 6/150\n",
      "77/77 [==============================] - 8s 108ms/step - loss: 13.4860\n",
      "Epoch 7/150\n",
      "77/77 [==============================] - 8s 107ms/step - loss: 13.6422\n",
      "Epoch 8/150\n",
      "77/77 [==============================] - 8s 106ms/step - loss: 13.6212\n",
      "Epoch 9/150\n",
      "77/77 [==============================] - 8s 107ms/step - loss: 13.0164\n",
      "Epoch 10/150\n",
      "77/77 [==============================] - 12s 154ms/step - loss: 13.3576\n",
      "Epoch 11/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 13.4080\n",
      "Epoch 12/150\n",
      "77/77 [==============================] - 8s 106ms/step - loss: 13.3292\n",
      "Epoch 13/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 13.4249\n",
      "Epoch 14/150\n",
      "77/77 [==============================] - 8s 107ms/step - loss: 13.2499\n",
      "Epoch 15/150\n",
      "77/77 [==============================] - 8s 108ms/step - loss: 13.3491\n",
      "Epoch 16/150\n",
      "77/77 [==============================] - 9s 111ms/step - loss: 13.2545\n",
      "Epoch 17/150\n",
      "77/77 [==============================] - 9s 111ms/step - loss: 13.5762\n",
      "Epoch 18/150\n",
      "77/77 [==============================] - 8s 106ms/step - loss: 13.3561\n",
      "Epoch 19/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 13.3734\n",
      "Epoch 20/150\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 13.2667\n",
      "Epoch 21/150\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 12.9145\n",
      "Epoch 22/150\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 13.0343\n",
      "Epoch 23/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.3225\n",
      "Epoch 24/150\n",
      "77/77 [==============================] - 10s 131ms/step - loss: 13.4287\n",
      "Epoch 25/150\n",
      "77/77 [==============================] - 8s 110ms/step - loss: 13.0600\n",
      "Epoch 26/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.5021\n",
      "Epoch 27/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.4104\n",
      "Epoch 28/150\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 13.1838\n",
      "Epoch 29/150\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 12.7890\n",
      "Epoch 30/150\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 13.0093\n",
      "Epoch 31/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 13.1000\n",
      "Epoch 32/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 13.0664\n",
      "Epoch 33/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.0473\n",
      "Epoch 34/150\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 13.1404\n",
      "Epoch 35/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.3244\n",
      "Epoch 36/150\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 13.1684\n",
      "Epoch 37/150\n",
      "77/77 [==============================] - 12s 152ms/step - loss: 13.0345\n",
      "Epoch 38/150\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 13.2811\n",
      "Epoch 39/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.9693\n",
      "Epoch 40/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.2454\n",
      "Epoch 41/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 13.2534\n",
      "Epoch 42/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.0557\n",
      "Epoch 43/150\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 13.0347\n",
      "Epoch 44/150\n",
      "77/77 [==============================] - 8s 108ms/step - loss: 12.9568\n",
      "Epoch 45/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 12.8952\n",
      "Epoch 46/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.9642\n",
      "Epoch 47/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.9787\n",
      "Epoch 48/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 12.8503\n",
      "Epoch 49/150\n",
      "77/77 [==============================] - 11s 145ms/step - loss: 13.0465\n",
      "Epoch 50/150\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 13.0161\n",
      "Epoch 51/150\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.1494\n",
      "Epoch 52/150\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 12.8806\n",
      "Epoch 53/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 12.8173\n",
      "Epoch 54/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.0028\n",
      "Epoch 55/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.0598\n",
      "Epoch 56/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.0964\n",
      "Epoch 57/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 12.9862\n",
      "Epoch 58/150\n",
      "77/77 [==============================] - 8s 106ms/step - loss: 13.1712\n",
      "Epoch 59/150\n",
      "77/77 [==============================] - 9s 111ms/step - loss: 12.8923\n",
      "Epoch 60/150\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 13.0499\n",
      "Epoch 61/150\n",
      "77/77 [==============================] - 8s 106ms/step - loss: 12.8596\n",
      "Epoch 62/150\n",
      "77/77 [==============================] - 8s 106ms/step - loss: 12.9480\n",
      "Epoch 63/150\n",
      "77/77 [==============================] - 12s 153ms/step - loss: 12.8671\n",
      "Epoch 64/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.1439\n",
      "Epoch 65/150\n",
      "77/77 [==============================] - 8s 106ms/step - loss: 12.7799\n",
      "Epoch 66/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 12.9856\n",
      "Epoch 67/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 12.9920\n",
      "Epoch 68/150\n",
      "77/77 [==============================] - 8s 106ms/step - loss: 12.9427\n",
      "Epoch 69/150\n",
      "77/77 [==============================] - 9s 111ms/step - loss: 12.9557\n",
      "Epoch 70/150\n",
      "77/77 [==============================] - 8s 106ms/step - loss: 12.9694\n",
      "Epoch 71/150\n",
      "77/77 [==============================] - 8s 106ms/step - loss: 12.6536\n",
      "Epoch 72/150\n",
      "77/77 [==============================] - 8s 107ms/step - loss: 12.9242\n",
      "Epoch 73/150\n",
      "77/77 [==============================] - 8s 107ms/step - loss: 12.9754\n",
      "Epoch 74/150\n",
      "77/77 [==============================] - 8s 109ms/step - loss: 12.4857\n",
      "Epoch 75/150\n",
      "77/77 [==============================] - 8s 109ms/step - loss: 12.8127\n",
      "Epoch 76/150\n",
      "77/77 [==============================] - 12s 152ms/step - loss: 12.9562\n",
      "Epoch 77/150\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 12.9317\n",
      "Epoch 78/150\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.9188\n",
      "Epoch 79/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.6836\n",
      "Epoch 80/150\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 12.8104\n",
      "Epoch 81/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 12.8528\n",
      "Epoch 82/150\n",
      "77/77 [==============================] - 8s 107ms/step - loss: 12.3757\n",
      "Epoch 83/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 13.1606\n",
      "Epoch 84/150\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 12.8419\n",
      "Epoch 85/150\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 12.9523\n",
      "Epoch 86/150\n",
      "77/77 [==============================] - 8s 109ms/step - loss: 12.8085\n",
      "Epoch 87/150\n",
      "77/77 [==============================] - 12s 154ms/step - loss: 12.6919\n",
      "Epoch 88/150\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 12.9918\n",
      "Epoch 89/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 12.7100\n",
      "Epoch 90/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 12.8469\n",
      "Epoch 91/150\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 12.7719\n",
      "Epoch 92/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 13.1038\n",
      "Epoch 93/150\n",
      "77/77 [==============================] - 9s 112ms/step - loss: 12.7142\n",
      "Epoch 94/150\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.8025\n",
      "Epoch 95/150\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.9051\n",
      "Epoch 96/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 13.0463\n",
      "Epoch 97/150\n",
      "77/77 [==============================] - 8s 108ms/step - loss: 12.9131\n",
      "Epoch 98/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 8s 107ms/step - loss: 12.7565\n",
      "Epoch 99/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.9586\n",
      "Epoch 100/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.9489\n",
      "Epoch 101/150\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.8380\n",
      "Epoch 102/150\n",
      "77/77 [==============================] - 12s 150ms/step - loss: 12.6387\n",
      "Epoch 103/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.8969\n",
      "Epoch 104/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.9228\n",
      "Epoch 105/150\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 12.9993\n",
      "Epoch 106/150\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.4767\n",
      "Epoch 107/150\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.7895\n",
      "Epoch 108/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 12.6365\n",
      "Epoch 109/150\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 12.7899\n",
      "Epoch 110/150\n",
      "77/77 [==============================] - 8s 107ms/step - loss: 12.7146\n",
      "Epoch 111/150\n",
      "77/77 [==============================] - 8s 106ms/step - loss: 12.7466\n",
      "Epoch 112/150\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 12.6833\n",
      "Epoch 113/150\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 12.7377\n",
      "Epoch 114/150\n",
      "77/77 [==============================] - 10s 136ms/step - loss: 12.9570\n",
      "Epoch 115/150\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.5542\n",
      "Epoch 116/150\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 12.6994\n",
      "Epoch 117/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.7958\n",
      "Epoch 118/150\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.9316\n",
      "Epoch 119/150\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 12.8262\n",
      "Epoch 120/150\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 12.5917\n",
      "Epoch 121/150\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.5325\n",
      "Epoch 122/150\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.7287\n",
      "Epoch 123/150\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 12.6696\n",
      "Epoch 124/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 13.0685\n",
      "Epoch 125/150\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 12.7701\n",
      "Epoch 126/150\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.7005\n",
      "Epoch 127/150\n",
      "77/77 [==============================] - 9s 113ms/step - loss: 12.7536\n",
      "Epoch 128/150\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 12.5469\n",
      "Epoch 129/150\n",
      "77/77 [==============================] - 11s 141ms/step - loss: 12.7077\n",
      "Epoch 130/150\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 12.5834\n",
      "Epoch 131/150\n",
      "77/77 [==============================] - 8s 108ms/step - loss: 12.6344\n",
      "Epoch 132/150\n",
      "77/77 [==============================] - 8s 106ms/step - loss: 12.8221\n",
      "Epoch 133/150\n",
      "77/77 [==============================] - 8s 108ms/step - loss: 12.6359\n",
      "Epoch 134/150\n",
      "77/77 [==============================] - 8s 109ms/step - loss: 13.0331\n",
      "Epoch 135/150\n",
      "77/77 [==============================] - 8s 108ms/step - loss: 12.7008\n",
      "Epoch 136/150\n",
      "77/77 [==============================] - 8s 107ms/step - loss: 12.6218\n",
      "Epoch 137/150\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.6347\n",
      "Epoch 138/150\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 12.6986\n",
      "Epoch 139/150\n",
      "77/77 [==============================] - 8s 106ms/step - loss: 12.8727\n",
      "Epoch 140/150\n",
      "77/77 [==============================] - 8s 107ms/step - loss: 12.4438\n",
      "Epoch 141/150\n",
      "77/77 [==============================] - 8s 106ms/step - loss: 12.6123\n",
      "Epoch 142/150\n",
      "77/77 [==============================] - 15s 195ms/step - loss: 12.6671\n",
      "Epoch 143/150\n",
      "77/77 [==============================] - 8s 107ms/step - loss: 12.6526\n",
      "Epoch 144/150\n",
      "77/77 [==============================] - 8s 109ms/step - loss: 12.8791\n",
      "Epoch 145/150\n",
      "77/77 [==============================] - 8s 108ms/step - loss: 12.6973\n",
      "Epoch 146/150\n",
      "77/77 [==============================] - 8s 107ms/step - loss: 12.5389\n",
      "Epoch 147/150\n",
      "77/77 [==============================] - 8s 107ms/step - loss: 12.4764\n",
      "Epoch 148/150\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 12.7355\n",
      "Epoch 149/150\n",
      "77/77 [==============================] - 8s 106ms/step - loss: 12.6097\n",
      "Epoch 150/150\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.5175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f46cc384220>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 150\n",
    "pre_trained_model = load_pretrained_model(pre_trained_loc='./PCLR.h5')\n",
    "latent = tf.keras.Model(pre_trained_model.inputs, pre_trained_model.get_layer('embed').output)\n",
    "full_model = AppendNet(latent, new_layers = [128, 1], classification=False)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam() # can modify LR, of course\n",
    "# loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "loss_fn = rmse_loss\n",
    "full_model.compile(optimizer, loss_fn)\n",
    "full_model.fit(X_train, y_train, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3c1690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "77/77 [==============================] - 11s 96ms/step - loss: 15.3669\n",
      "Epoch 2/200\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 13.9606\n",
      "Epoch 3/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 13.8542\n",
      "Epoch 4/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 13.3122\n",
      "Epoch 5/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 13.6303\n",
      "Epoch 6/200\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 13.5146\n",
      "Epoch 7/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 13.7146\n",
      "Epoch 8/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.6542\n",
      "Epoch 9/200\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 13.0197\n",
      "Epoch 10/200\n",
      "77/77 [==============================] - 8s 97ms/step - loss: 13.3400\n",
      "Epoch 11/200\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 13.4389\n",
      "Epoch 12/200\n",
      "77/77 [==============================] - 9s 106ms/step - loss: 13.3288\n",
      "Epoch 13/200\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 13.4125\n",
      "Epoch 14/200\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 13.2795\n",
      "Epoch 15/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 13.4566\n",
      "Epoch 16/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 13.3196\n",
      "Epoch 17/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 13.6173\n",
      "Epoch 18/200\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 13.4151\n",
      "Epoch 19/200\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 13.4309\n",
      "Epoch 20/200\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 13.3011\n",
      "Epoch 21/200\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.9506\n",
      "Epoch 22/200\n",
      "77/77 [==============================] - 11s 146ms/step - loss: 13.0803\n",
      "Epoch 23/200\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.3447\n",
      "Epoch 24/200\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 13.4554\n",
      "Epoch 25/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.0658\n",
      "Epoch 26/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 13.4615\n",
      "Epoch 27/200\n",
      "77/77 [==============================] - 8s 97ms/step - loss: 13.4341\n",
      "Epoch 28/200\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 13.1985\n",
      "Epoch 29/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.7908\n",
      "Epoch 30/200\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 12.9918\n",
      "Epoch 31/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 13.1425\n",
      "Epoch 32/200\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 13.0759\n",
      "Epoch 33/200\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 13.0895\n",
      "Epoch 34/200\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 13.1590\n",
      "Epoch 35/200\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 13.3369\n",
      "Epoch 36/200\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.1708\n",
      "Epoch 37/200\n",
      "77/77 [==============================] - 10s 132ms/step - loss: 13.0487\n",
      "Epoch 38/200\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 13.3247\n",
      "Epoch 39/200\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 12.9855\n",
      "Epoch 40/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.2290\n",
      "Epoch 41/200\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.2269\n",
      "Epoch 42/200\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 13.0628\n",
      "Epoch 43/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.0220\n",
      "Epoch 44/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.9554\n",
      "Epoch 45/200\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 12.8989\n",
      "Epoch 46/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.9797\n",
      "Epoch 47/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 12.9590\n",
      "Epoch 48/200\n",
      "77/77 [==============================] - 10s 134ms/step - loss: 12.8326\n",
      "Epoch 49/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.0799\n",
      "Epoch 50/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.0125\n",
      "Epoch 51/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.1068\n",
      "Epoch 52/200\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 12.9180\n",
      "Epoch 53/200\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 12.7984\n",
      "Epoch 54/200\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 12.9902\n",
      "Epoch 55/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.0682\n",
      "Epoch 56/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 13.1234\n",
      "Epoch 57/200\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 13.0302\n",
      "Epoch 58/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.2201\n",
      "Epoch 59/200\n",
      "77/77 [==============================] - 11s 144ms/step - loss: 12.9045\n",
      "Epoch 60/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.0672\n",
      "Epoch 61/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.9034\n",
      "Epoch 62/200\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.9712\n",
      "Epoch 63/200\n",
      "77/77 [==============================] - 8s 108ms/step - loss: 12.8636\n",
      "Epoch 64/200\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 13.1179\n",
      "Epoch 65/200\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 12.7663\n",
      "Epoch 66/200\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.9987\n",
      "Epoch 67/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 13.0333\n",
      "Epoch 68/200\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 12.9559\n",
      "Epoch 69/200\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 13.0049\n",
      "Epoch 70/200\n",
      "77/77 [==============================] - 10s 134ms/step - loss: 13.0218\n",
      "Epoch 71/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 12.6615\n",
      "Epoch 72/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.9418\n",
      "Epoch 73/200\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.0191\n",
      "Epoch 74/200\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 12.5429\n",
      "Epoch 75/200\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 12.8583\n",
      "Epoch 76/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.9177\n",
      "Epoch 77/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.9905\n",
      "Epoch 78/200\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.9345\n",
      "Epoch 79/200\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.7194\n",
      "Epoch 80/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.8138\n",
      "Epoch 81/200\n",
      "77/77 [==============================] - 10s 135ms/step - loss: 12.8282\n",
      "Epoch 82/200\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.4202\n",
      "Epoch 83/200\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 13.1992\n",
      "Epoch 84/200\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.8535\n",
      "Epoch 85/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.9056\n",
      "Epoch 86/200\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.7932\n",
      "Epoch 87/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 12.6993\n",
      "Epoch 88/200\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 13.0400\n",
      "Epoch 89/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 12.7352\n",
      "Epoch 90/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 12.8748\n",
      "Epoch 91/200\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 12.7402\n",
      "Epoch 92/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 13.1537\n",
      "Epoch 93/200\n",
      "77/77 [==============================] - 10s 135ms/step - loss: 12.7367\n",
      "Epoch 94/200\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.8176\n",
      "Epoch 95/200\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.9530\n",
      "Epoch 96/200\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 13.0396\n",
      "Epoch 97/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.9114\n",
      "Epoch 98/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.7452\n",
      "Epoch 99/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 8s 100ms/step - loss: 12.9548\n",
      "Epoch 100/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 12.9289\n",
      "Epoch 101/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 12.8555\n",
      "Epoch 102/200\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 12.6467\n",
      "Epoch 103/200\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 12.9192\n",
      "Epoch 104/200\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 12.8706\n",
      "Epoch 105/200\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 12.9904\n",
      "Epoch 106/200\n",
      "77/77 [==============================] - 10s 129ms/step - loss: 12.4944\n",
      "Epoch 107/200\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 12.8021\n",
      "Epoch 108/200\n",
      "77/77 [==============================] - 7s 94ms/step - loss: 12.6268\n",
      "Epoch 109/200\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 12.8231\n",
      "Epoch 110/200\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 12.7769\n",
      "Epoch 111/200\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 12.7922\n",
      "Epoch 112/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 12.6598\n",
      "Epoch 113/200\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 12.7137\n",
      "Epoch 114/200\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 12.9442\n",
      "Epoch 115/200\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 12.5476\n",
      "Epoch 116/200\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 12.7122\n",
      "Epoch 117/200\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 12.7808\n",
      "Epoch 118/200\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 12.9253\n",
      "Epoch 119/200\n",
      "77/77 [==============================] - 10s 130ms/step - loss: 12.8353\n",
      "Epoch 120/200\n",
      "77/77 [==============================] - 8s 97ms/step - loss: 12.5778\n",
      "Epoch 121/200\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 12.5351\n",
      "Epoch 122/200\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 12.7147\n",
      "Epoch 123/200\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 12.6593\n",
      "Epoch 124/200\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 13.0487\n",
      "Epoch 125/200\n",
      "77/77 [==============================] - 7s 95ms/step - loss: 12.7470\n",
      "Epoch 126/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 12.6584\n",
      "Epoch 127/200\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 12.7758\n",
      "Epoch 128/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.5522\n",
      "Epoch 129/200\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.7446\n",
      "Epoch 130/200\n",
      "77/77 [==============================] - 8s 108ms/step - loss: 12.5613\n",
      "Epoch 131/200\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 12.6449\n",
      "Epoch 132/200\n",
      "77/77 [==============================] - 10s 129ms/step - loss: 12.8518\n",
      "Epoch 133/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 12.6648\n",
      "Epoch 134/200\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 13.0537\n",
      "Epoch 135/200\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 12.6967\n",
      "Epoch 136/200\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 12.6303\n",
      "Epoch 137/200\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 12.6569\n",
      "Epoch 138/200\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 12.6890\n",
      "Epoch 139/200\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 12.8442\n",
      "Epoch 140/200\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 12.4058\n",
      "Epoch 141/200\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 12.6342\n",
      "Epoch 142/200\n",
      "77/77 [==============================] - 7s 95ms/step - loss: 12.6746\n",
      "Epoch 143/200\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 12.6621\n",
      "Epoch 144/200\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 12.8343\n",
      "Epoch 145/200\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 12.7250\n",
      "Epoch 146/200\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.5608\n",
      "Epoch 147/200\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 12.4600\n",
      "Epoch 148/200\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 12.7460\n",
      "Epoch 149/200\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.6302\n",
      "Epoch 150/200\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.5131\n",
      "Epoch 151/200\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 12.8309\n",
      "Epoch 152/200\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 12.7528\n",
      "Epoch 153/200\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.4455\n",
      "Epoch 154/200\n",
      "77/77 [==============================] - 8s 105ms/step - loss: 12.7533\n",
      "Epoch 155/200\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 12.7026\n",
      "Epoch 156/200\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.4517\n",
      "Epoch 157/200\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 12.3760\n",
      "Epoch 158/200\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 12.7325\n",
      "Epoch 159/200\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 12.7303\n",
      "Epoch 160/200\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 12.8125\n",
      "Epoch 161/200\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.7382\n",
      "Epoch 162/200\n",
      "77/77 [==============================] - 9s 112ms/step - loss: 12.5734\n",
      "Epoch 163/200\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.6804\n",
      "Epoch 164/200\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.7777\n",
      "Epoch 165/200\n",
      "77/77 [==============================] - 8s 103ms/step - loss: 12.4939\n",
      "Epoch 166/200\n",
      "63/77 [=======================>......] - ETA: 1s - loss: 12.6570"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "pre_trained_model = load_pretrained_model(pre_trained_loc='./PCLR.h5')\n",
    "latent = tf.keras.Model(pre_trained_model.inputs, pre_trained_model.get_layer('embed').output)\n",
    "full_model = AppendNet(latent, new_layers = [128, 1], classification=False)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam() # can modify LR, of course\n",
    "# loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "loss_fn = rmse_loss\n",
    "full_model.compile(optimizer, loss_fn)\n",
    "full_model.fit(X_train, y_train, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded6a47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Getting prediction result from test set\n",
    "'''\n",
    "y_pred = full_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f9c402ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./PCLR_finetuned_100epc.pb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./PCLR_finetuned_100epc.pb/assets\n"
     ]
    }
   ],
   "source": [
    "full_model.save('./PCLR_finetuned_100epc.pb', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da385cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved tf model\n",
    "loaded_model = tf.keras.models.load_model('./PCLR_finetuned.pb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab0b27b",
   "metadata": {},
   "source": [
    "# Calculate Classification Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d07769dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(11.718298284532933, shape=(), dtype=float64)\n",
      "0.5179734023757416 1.7000212654359237e-64\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "r, pval = stats.pearsonr(y_test, np.concatenate(y_pred))\n",
    "print(rmse_loss(y_test, y_pred))\n",
    "print(r, pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "689e3810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.662241673856977 0.5168803322877793 3.885793744767385e-48\n",
      "(10.168321327256356, 11.186809058909072) (0.46935757473184414, 0.5653645253170076) (4.403350490234938e-79, 9.551261205355765e-52)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "50 epoch model\n",
    "'''\n",
    "rmse, r, pval = do_bootstrap_regression(np.concatenate(y_pred), y_test)\n",
    "print(rmse.mean(), r.mean(), pval.mean())\n",
    "print(confidence_interval(rmse), confidence_interval(r), confidence_interval(pval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ede983b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.08667670973824 0.5084383288648698 5.124600599466284e-44\n",
      "(9.593113688193553, 10.608999002632787) (0.4534978599475333, 0.5592618550246696) (4.559193565980286e-77, 5.1785051956098e-48)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "100 epoch model\n",
    "'''\n",
    "rmse, r, pval = do_bootstrap_regression(np.concatenate(y_pred), y_test)\n",
    "print(rmse.mean(), r.mean(), pval.mean())\n",
    "print(confidence_interval(rmse), confidence_interval(r), confidence_interval(pval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c4ea977b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.848489722993618 0.46944465590783035 3.0449579264607663e-35\n",
      "(9.338509122190928, 10.366695124278358) (0.4179658931128352, 0.5250832141152932) (1.53915223843851e-66, 2.4962666858646983e-40)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "150 epoch model\n",
    "'''\n",
    "rmse, r, pval = do_bootstrap_regression(np.concatenate(y_pred), y_test)\n",
    "print(rmse.mean(), r.mean(), pval.mean())\n",
    "print(confidence_interval(rmse), confidence_interval(r), confidence_interval(pval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f5696",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "200 epoch model\n",
    "'''\n",
    "rmse, r, pval = do_bootstrap_regression(np.concatenate(y_pred), y_test)\n",
    "print(rmse.mean(), r.mean(), pval.mean())\n",
    "print(confidence_interval(rmse), confidence_interval(r), confidence_interval(pval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86d21a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.739295 , 16.536602 , 12.847801 ,  5.965122 ,  7.54178  ,\n",
       "       14.392495 ,  7.631265 ,  8.851204 ,  5.6282945,  4.4882045,\n",
       "        8.637509 ,  9.091559 , 12.68519  ,  4.9804873,  9.919694 ,\n",
       "        9.347183 , 12.241269 ,  8.51965  ,  6.159464 ,  8.667086 ,\n",
       "       10.11269  ,  5.9755054,  8.68156  , 12.302162 , 12.5901985,\n",
       "       10.339763 , 14.327306 , 13.476693 ,  7.008696 ,  8.134004 ,\n",
       "        8.393402 ,  8.060931 , 13.770674 ,  6.4146132,  8.829255 ,\n",
       "        9.263454 ,  8.681663 ,  5.6111755,  6.4762654, 14.283297 ,\n",
       "        8.735974 ,  6.060723 ,  6.7307086, 10.041674 , 11.156607 ,\n",
       "       10.237392 ,  7.815382 , 15.611364 ,  9.543268 , 15.904237 ,\n",
       "       11.327438 ,  9.46977  ,  7.2899356,  4.0003815,  2.8161058,\n",
       "        2.6849854, 10.304205 , 10.832106 , 11.151959 ,  5.200855 ,\n",
       "        8.322233 ,  5.21683  ,  3.334819 , 17.38361  , 12.258114 ,\n",
       "        8.287039 ,  8.402273 ,  6.645752 ,  5.654884 ,  5.3012967,\n",
       "        8.535572 , 10.340702 ,  6.2664638,  5.840609 ,  7.5901203,\n",
       "        4.4116454,  7.0098042, 14.621322 ,  3.9870303, 10.332638 ,\n",
       "        8.665495 , 11.955424 ,  7.678335 ,  8.495907 ,  9.829441 ,\n",
       "       10.587822 ,  6.116463 ,  9.408282 , 11.17298  , 10.554146 ,\n",
       "        8.795747 ,  7.9876685,  5.483663 ,  8.949944 , 10.984779 ,\n",
       "        8.9731865, 10.470873 ,  8.22606  ,  6.826321 , 11.305021 ,\n",
       "       11.873709 ,  3.9500868,  2.6510837,  5.399363 ,  8.343295 ,\n",
       "       10.12257  , 10.799257 ,  4.75984  , 10.017008 ,  6.3140173,\n",
       "       10.616758 ,  5.195718 ,  8.3739   , 11.144119 ,  7.055556 ,\n",
       "        8.800832 ,  4.5861177,  9.180487 ,  6.4841213,  7.679766 ,\n",
       "        7.2223   ,  8.193858 ,  6.789661 ,  6.399362 ,  9.911248 ,\n",
       "       16.286905 ,  5.3000693,  4.2012143,  9.756771 ,  4.560769 ,\n",
       "       17.276493 ,  9.9933   ,  8.371095 , 13.787298 ,  9.105486 ,\n",
       "       11.296871 , 13.578508 ,  7.0249043,  9.577476 , 10.263407 ,\n",
       "       13.589507 ,  9.087931 , 11.02381  ,  4.4274797,  4.110165 ,\n",
       "        4.264366 ,  4.7127795,  4.852412 ,  5.757443 ,  4.805427 ,\n",
       "        4.282984 ,  5.0693216,  4.8829155,  4.8945637,  4.217864 ,\n",
       "        5.150322 ,  5.0675917,  4.609273 ,  4.3172174,  9.697572 ,\n",
       "        7.6007524,  8.728583 ,  6.3703394, 11.288717 ,  8.10935  ,\n",
       "        6.03165  ,  7.5830016,  6.270619 ,  6.049846 ,  4.7571115,\n",
       "        4.872536 ,  4.975808 ,  5.965073 ,  6.1322064,  6.907532 ,\n",
       "        4.993835 ,  7.4151616, 10.608756 ,  9.128529 , 14.2337   ,\n",
       "        8.133197 ,  4.4626684,  5.176794 ,  5.3517017,  8.777914 ,\n",
       "       10.059664 ,  8.317755 ,  7.0169673,  7.0930247,  7.020459 ,\n",
       "        7.4675584,  7.6962323,  5.9274755,  5.6341195,  7.2389674,\n",
       "        6.000941 ,  6.5436287,  6.834353 ,  6.081036 ,  5.457015 ,\n",
       "        9.766413 ,  6.430074 , 11.845402 , 12.9264965,  7.9275346,\n",
       "       11.0567   ,  8.357057 , 11.857948 , 12.390924 , 10.257103 ,\n",
       "       12.332484 ,  5.6099052,  4.0736656,  5.0985484,  3.9031825,\n",
       "        4.08847  ,  4.398212 ,  5.600632 ,  2.740475 ,  4.6115894,\n",
       "        4.10277  ,  2.9311583,  4.5094466,  7.393375 ,  8.219046 ,\n",
       "        8.981366 ,  7.592254 ,  6.681509 ,  7.8533883,  6.70441  ,\n",
       "        7.869304 ,  7.03224  , 10.191435 ,  6.0800347,  5.415795 ,\n",
       "        9.369706 ,  5.6289144,  7.543648 ,  6.3911185,  6.3947935,\n",
       "        6.4523816,  5.8160887,  6.299043 ,  7.158299 ,  7.7586846,\n",
       "        5.025162 ,  7.312605 ,  4.4879484,  5.968371 ,  5.257089 ,\n",
       "        6.122007 ,  4.316772 ,  9.091122 ,  7.44679  ,  9.738057 ,\n",
       "        6.021918 ,  8.55837  ,  4.478518 ,  5.2191453,  9.894806 ,\n",
       "        7.594068 ,  7.5923543,  7.475541 , 10.837503 , 11.193226 ,\n",
       "       11.669939 , 13.629446 ,  5.0961885,  3.6104815,  4.2445207,\n",
       "        5.486172 ,  4.332922 ,  5.153108 ,  4.840491 ,  4.2970233,\n",
       "        5.0917726,  4.295732 ,  3.922551 ,  4.112018 ,  4.031706 ,\n",
       "        4.7509756,  4.755176 ,  5.608637 ,  4.735657 ,  5.0563493,\n",
       "        4.093369 ,  4.545813 ,  4.730462 ,  3.8937256,  4.80847  ,\n",
       "        4.1246734,  4.175675 ,  4.4050965,  9.790044 ,  4.3219223,\n",
       "        8.161546 , 10.761332 ,  8.91458  ,  9.409981 ,  7.0683656,\n",
       "        7.7196097,  9.90494  , 10.339535 , 10.887531 ,  5.8081903,\n",
       "        6.143213 ,  4.9505687, 12.074354 , 10.983149 ,  6.9587064,\n",
       "       11.329692 ,  7.552859 ,  9.963034 , 11.773003 , 11.498423 ,\n",
       "        4.6480064,  4.3788915, 12.1027775,  5.7595363,  7.251972 ,\n",
       "        6.340487 ,  9.981161 ,  6.7178273,  8.587105 ,  8.544006 ,\n",
       "       14.348734 ,  7.2688184, 10.449936 ,  5.78397  ,  9.143003 ,\n",
       "        8.594879 ,  6.9679694,  8.658454 ,  8.132173 , 16.324873 ,\n",
       "        4.186001 ,  4.44848  ,  4.5584326,  4.5927496,  5.4668317,\n",
       "        9.6130905,  9.329128 ,  3.9529374,  9.841823 ,  9.934775 ,\n",
       "        9.851485 , 10.1024475,  9.261628 ,  9.579334 ,  9.681692 ,\n",
       "        5.804421 ,  5.0890183,  6.161339 ,  5.24793  ,  4.3749666,\n",
       "        4.45501  ,  4.771338 ,  4.341575 ,  5.2300534,  3.8091867,\n",
       "        6.164467 ,  9.219514 ,  5.325948 ,  5.627011 ,  5.2185893,\n",
       "        5.7152762,  5.3099365,  3.3403437,  4.036783 ,  3.9580696,\n",
       "        4.5635834,  5.2492743,  3.8651755,  6.6792946,  4.8765492,\n",
       "        5.0952687,  4.724304 ,  4.3377633, 10.658344 ,  4.656867 ,\n",
       "        3.8000424,  5.46418  ,  4.22741  ,  3.2865927,  3.5801346,\n",
       "        8.971087 ,  7.5620666,  7.9980245,  8.254281 ,  8.076575 ,\n",
       "        8.473106 ,  3.8906863,  2.1767042,  6.3699265,  5.186035 ,\n",
       "       10.834365 ,  8.159347 ,  8.772194 , 12.112235 ,  9.424895 ,\n",
       "       12.929583 ,  4.867914 ,  6.47862  ,  6.257631 ,  5.931134 ,\n",
       "        5.7022243,  5.914988 ,  4.180768 ,  4.143861 ,  4.5105343,\n",
       "        6.1384826,  5.1093974,  4.5516667,  5.3492007, 13.046749 ,\n",
       "       13.954742 , 12.864854 , 11.183854 ,  5.8194385,  5.1794915,\n",
       "        9.682468 ,  7.4076266, 10.381628 ,  3.6868808,  3.615246 ,\n",
       "        3.1983883,  3.7838886,  3.0931826,  3.9988627,  3.5091271,\n",
       "        3.594731 ,  3.8447864,  4.924738 ,  8.715636 , 10.1681   ,\n",
       "        6.405723 ,  9.09385  ,  8.114047 ,  9.364521 ,  5.2499843,\n",
       "        7.9690223,  6.5255075,  7.067288 ,  6.2283177,  5.5825477,\n",
       "        7.048713 ,  6.342014 ,  6.567695 ,  6.0751195,  8.49292  ,\n",
       "        5.9135566,  5.546465 ,  6.3382797,  4.3245163,  5.500048 ,\n",
       "        6.0740323,  6.197378 ,  5.341681 ,  6.2734113,  6.894877 ,\n",
       "        6.3137164,  6.509871 ,  7.9525104,  8.111191 ,  8.545271 ,\n",
       "        7.735987 ,  8.404582 ,  6.139338 ,  6.0592   ,  5.453586 ,\n",
       "        7.6646247,  6.6199813,  4.150176 , 11.540777 ,  3.5431497,\n",
       "        4.0339823,  4.519104 ,  4.2094374,  4.098421 ,  3.6229074,\n",
       "        4.628104 ,  3.5598588,  3.8229873,  3.4228559,  3.3984   ,\n",
       "        4.221573 ,  2.6675982,  7.26032  , 13.363236 ,  7.9458156,\n",
       "        9.365637 ,  7.317247 ,  6.0477433,  5.8047476,  5.7373185,\n",
       "        5.74964  ,  5.8326225,  5.6129436,  5.18578  ,  5.3607435,\n",
       "        4.9543567,  5.376099 ,  4.219871 ,  5.3003507,  4.430425 ,\n",
       "        5.858046 ,  5.219478 ,  5.2056093,  3.9443367,  3.7015195,\n",
       "        3.6046379,  4.154717 ,  3.186646 ,  3.6258585,  4.1603694,\n",
       "        3.8720396,  3.423588 ,  3.4173849,  3.916968 ,  5.762317 ,\n",
       "        4.3610287,  3.5705926,  3.059734 ,  7.3600917,  7.7927833,\n",
       "        5.0923295,  9.42057  ,  8.73713  ,  8.429545 ,  9.427355 ,\n",
       "        8.09497  ,  7.8774595,  8.623269 , 11.906786 ,  9.525682 ,\n",
       "       12.767118 ,  8.011602 ,  8.227508 ,  7.2383156, 10.615196 ,\n",
       "        8.670474 , 10.567278 , 10.242903 , 12.327434 ,  9.998631 ,\n",
       "       15.372877 , 11.970057 , 16.654661 , 12.009111 ,  8.664943 ,\n",
       "        5.78897  ,  6.24161  ,  7.405816 ,  6.272077 ,  7.6849327,\n",
       "        7.4568167,  6.461079 ,  6.8201957,  6.092074 ,  8.973005 ,\n",
       "        8.9395485,  9.000635 ,  5.521885 ,  5.3152604,  7.342032 ,\n",
       "        7.404837 ,  9.057736 , 11.64012  , 10.871038 ,  6.3050466,\n",
       "        8.579478 ,  6.165157 ,  4.6332097,  4.3849597,  4.560812 ,\n",
       "        4.59226  ,  4.1574283,  4.4988537,  4.578343 ,  4.368485 ,\n",
       "        4.4050417,  4.6839776,  5.0006604,  5.5979195,  6.1527677,\n",
       "        7.1659203,  9.989643 , 11.748868 ,  6.380348 ,  6.0123496,\n",
       "        6.6764073,  7.139485 ,  5.659261 ,  6.5172887,  5.5414486,\n",
       "        6.7567954,  5.844439 ,  6.2638063,  6.5178485,  5.7278624,\n",
       "        5.646451 ,  6.00175  , 13.73428  ,  5.6124406,  6.2850537,\n",
       "        9.821469 ,  5.637296 , 13.908847 ,  3.4801877,  7.279965 ,\n",
       "       12.010912 ,  9.320001 , 11.9936905,  4.1625805,  4.4796433,\n",
       "        5.311276 ,  5.3698277,  4.9417987,  4.010129 ,  4.6943583,\n",
       "        5.261828 ,  4.513201 ,  5.143184 ,  4.2023697,  5.2613034,\n",
       "        4.014988 ,  4.640664 ,  4.0228777,  4.3130083,  6.4052796,\n",
       "        8.16187  ,  7.7561173,  8.072411 ,  9.214325 ,  7.96013  ,\n",
       "        7.187943 ,  6.8325706,  5.136703 ,  5.207507 ,  5.6148033,\n",
       "        5.365914 ,  5.6486106,  5.7025137,  6.113468 ,  4.578453 ,\n",
       "        4.608694 ,  4.286156 ,  7.3119254,  7.676028 , 11.408213 ,\n",
       "        9.339149 ,  8.184457 , 13.166901 ,  7.1076875,  8.812971 ,\n",
       "        5.6597037,  7.8109765,  7.377414 ,  8.386693 ,  9.38095  ,\n",
       "        8.00565  ,  8.285104 , 10.117126 , 11.673662 ,  4.832659 ,\n",
       "        8.731281 ,  6.568755 ,  7.014789 ,  6.279075 ,  8.290787 ,\n",
       "        7.810394 , 10.983957 , 13.228616 ,  5.6269374,  4.04134  ,\n",
       "        9.467573 ,  7.521782 , 12.651472 ,  8.01451  ,  8.9051895,\n",
       "        7.293769 ,  4.9571924,  8.844222 , 11.625013 , 11.396973 ,\n",
       "       14.991392 ,  5.561248 ,  7.946608 ,  9.215538 ,  7.7727876,\n",
       "        7.0590873,  6.2848487,  5.5830665, 10.761044 ,  7.8291097,\n",
       "        7.269359 ,  6.8791075, 14.294823 , 11.122253 , 10.07769  ,\n",
       "       12.209746 , 11.604694 ,  9.688442 , 12.681884 , 12.23447  ,\n",
       "       11.19898  ,  5.608158 ,  4.560787 ,  4.870978 ,  5.524741 ,\n",
       "        4.3693485,  3.551922 ,  4.70982  ,  4.0582914,  3.5553343,\n",
       "        4.3796196,  7.480319 , 11.467399 , 10.373283 ,  9.513651 ,\n",
       "        5.294104 ,  5.370287 ,  5.6120543,  6.178683 ,  5.4238524,\n",
       "        4.5850763,  4.7191296,  5.750424 ,  5.1824536,  5.4779506,\n",
       "        4.838026 ,  4.4888787, 13.669275 , 11.765488 ,  5.844277 ,\n",
       "        9.091737 ,  5.5633283,  7.015386 ,  5.9727244,  7.649646 ,\n",
       "        6.4163713,  6.9973164,  6.133912 ,  6.5729866,  6.2997203,\n",
       "        6.8147063,  6.144007 ,  6.681761 ,  6.129841 ,  7.523582 ,\n",
       "        6.702288 ,  5.873689 ,  6.7851176,  6.343812 ,  5.931309 ,\n",
       "        5.880196 ,  8.570307 ,  7.6297135,  7.6831985,  8.582719 ,\n",
       "       11.624398 ,  5.6750636,  7.86843  ,  7.985909 ,  8.912293 ,\n",
       "        2.4019284,  7.75437  ,  7.1654162, 11.6817   ,  5.43353  ,\n",
       "        4.9041905, 10.370226 ,  6.7071004, 11.364282 , 10.223602 ,\n",
       "        5.460422 ,  9.553791 ,  8.915313 , 12.637512 ,  5.7765055,\n",
       "        8.516539 ,  5.1025915, 17.521711 ,  5.551843 ,  6.211871 ,\n",
       "        4.865562 ,  6.793865 ,  4.367994 ,  4.8622932,  4.459932 ,\n",
       "        5.126904 ,  3.8787005,  6.1460757,  5.523713 ,  6.41174  ,\n",
       "        5.3574452,  5.5571094, 15.469433 , 12.973439 ,  6.006036 ,\n",
       "        6.147874 ,  5.1433244,  6.850608 , 11.011009 ,  9.33405  ,\n",
       "        5.0928826,  5.887524 ,  7.130439 ,  9.13563  ,  6.8452377,\n",
       "        4.2927265, 11.981522 , 11.554358 ,  7.5896173, 10.519284 ,\n",
       "        7.4573917,  8.557104 , 12.10914  ,  9.055548 ,  8.992096 ,\n",
       "       10.588182 , 11.229404 ,  5.554599 ,  6.3060465,  4.806978 ,\n",
       "        6.596357 ,  4.3294334,  5.5104604,  6.2473383,  5.274836 ,\n",
       "        4.1183887,  3.896476 ,  5.46223  ,  6.7765374,  6.249069 ,\n",
       "        5.8792505,  5.2470717,  8.736323 , 10.104902 ,  6.2487926,\n",
       "        7.522894 ,  8.309832 ,  8.376228 ,  8.187594 , 11.66125  ,\n",
       "        6.3366723,  8.478277 ,  5.3380704,  6.237266 ,  4.5766363,\n",
       "        6.1632743,  5.1693597,  7.5349326,  3.679332 ,  3.8092668,\n",
       "        3.7059393,  4.9210687,  2.714631 ,  4.005116 ,  3.3707628,\n",
       "        5.9983563,  8.629414 ,  7.84407  ,  5.922693 ,  7.0490212,\n",
       "        5.0317373,  5.256824 ,  4.343083 ,  5.4973035,  8.7791605,\n",
       "        6.391812 ,  7.472394 , 10.293716 , 11.07058  ,  9.355234 ,\n",
       "        7.844872 ,  5.43999  ,  4.5860066,  4.6341805,  4.0609517,\n",
       "        4.108123 ,  3.0195978,  3.5324776,  3.8515375,  3.6482282,\n",
       "        9.437191 , 11.667959 ,  7.0161023,  5.137861 ,  8.6362095,\n",
       "        9.871913 ,  5.2189455,  4.2198796,  4.056852 ,  3.7412632,\n",
       "        3.258642 ,  5.1736593,  4.1852937,  4.0965643,  4.1460967,\n",
       "        3.8557746,  3.8176527,  4.8347054,  4.6742454,  6.9834805,\n",
       "        6.4434543,  8.933861 ,  8.210613 ,  6.4409885,  9.690311 ,\n",
       "        7.8981442, 12.089487 ,  9.799059 ,  6.585005 ,  7.3047824,\n",
       "        8.966565 ,  8.506602 ,  7.9246163, 11.742303 ,  8.371502 ,\n",
       "       10.591506 ,  8.983686 ,  5.8644166], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0863fe14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
